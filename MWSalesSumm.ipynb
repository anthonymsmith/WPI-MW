{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Copyright (c) Nolichucky Associates 2024. All Rights Reserved.\n",
    "\n",
    "This software is the confidential and proprietary information of Nolichucky Associates.\n",
    "You shall not disclose such Confidential Information and shall use it only in accordance\n",
    " with the terms of the license agreement you entered into with Nolichucky Associates.\n",
    "\n",
    "Unauthorized copying of this file, via any medium, is strictly prohibited.\n",
    "Proprietary and confidential.\n",
    "\n",
    "Project: Music Worcester Patron and Event Analytics\n",
    "\n",
    "Author: Anthony Smith\n",
    "Date: September, 2024\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import display\n",
    "import MW_functions as mw\n",
    "import Model_functions as mod\n",
    "\n",
    "# set and change to working directory\n",
    "default_data_dir = r'c:\\Users\\antho\\WPI-MW'\n",
    "#data_dir = default_data_dir\n",
    "data_dir = input(\"Enter the name of the home directory: \") or default_data_dir\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Set default logging level and update based on user input\n",
    "default_log_level = 'INFO'\n",
    "user_input_log_level = input(\"Enter logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) [INFO]: \") or default_log_level\n",
    "numeric_level = getattr(logging, user_input_log_level.upper(), None)\n",
    "if not isinstance(numeric_level, int):\n",
    "    raise ValueError(f'Invalid log level: {user_input_log_level}')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    # Create log file handlers\n",
    "    f_handler = logging.FileHandler(os.path.join(data_dir, 'sales.log'), mode='w')\n",
    "    f_formatter = logging.Formatter('%(asctime)s - %(lineno)s - %(levelname)s - %(message)s')\n",
    "    f_handler.setFormatter(f_formatter)\n",
    "    f_handler.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(f_handler)\n",
    "\n",
    "    # Create log stream handlers\n",
    "    c_handler = logging.StreamHandler(sys.stdout)\n",
    "    c_formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "    c_handler.setFormatter(c_formatter)\n",
    "    c_handler.setLevel(numeric_level)\n",
    "    logger.addHandler(c_handler)\n",
    "logger.debug(f\"Effective logger level: {logger.getEffectiveLevel()}\")\n",
    "logger.debug(f'Working directory is: {data_dir}')\n",
    "\n",
    "# input file names\n",
    "manifest_file = 'EventManifest.xlsx' # Manually created with event attributes\n",
    "PnL_file = 'Budget\\\\EventPnL.xlsx' # Manually updated Event Profit and Loss file\n",
    "default_sales_file = 'SalesforceLatest.csv' # raw Saleforce input file\n",
    "AccountId_file = 'All Accounts Contacts IDs.csv' # currently not used.\n",
    "chorus_file = 'FinalChorusList.csv' # List of chorus members, manually created\n",
    "regions_file = 'final_regions.csv' # input file to define region mapping.\n",
    "\n",
    "# output file names\n",
    "PnLProcessed_file = 'Budget\\\\EventPnLProcessed.csv' # Merged event manifest and PnL data\n",
    "default_output_file = 'DataMerge.csv' # full processed transaction file\n",
    "output_file = default_output_file\n",
    "anon_output_file = 'anon_' + output_file # anonymized form\n",
    "\n",
    "#user_output_file = input(\"Enter the name of the output file: \") or default_output_file\n",
    "\n",
    "patron_input_file = output_file # Only used to avoid regenerating Lat/Long TODO: clean this up.\n",
    "patron_details_file = 'PatronDetails.csv' # output patron details\n",
    "anon_patron_file = 'anon_' + patron_details_file # anonymized form\n",
    "\n",
    "# Assign static parameters. NOTE: some of these could be reduced. \n",
    "usps_userid = '3MUSIC10O3813'\n",
    "RFMScoreThreshold = 0 # RFM score below which we don't look up lat/long\n",
    "getZIP = True\n",
    "yearsOfData = 15 # to keep\n",
    "new_threshold = 250 # days since First Event for determining \"New\"\n",
    "returning_threshold = 500 # days determining previous event for \"Returning\"\n",
    "\n",
    "# Prompt the user for the sales file\n",
    "sales_file = default_sales_file\n",
    "#sales_file = input(\"Enter the name of the sales file: \") or default_sales_fil\n",
    "\n",
    "logger.info(f'Input files are: {manifest_file},{PnL_file}, {sales_file}, {regions_file}')\n",
    "logger.info(f'Output files are: {PnLProcessed_file}, {output_file}')\n",
    "\n",
    "\"\"\"\n",
    "Function: main\n",
    "\n",
    "Description:\n",
    "    This is the main function that orchestrates the processing of event, sales, and financial data. \n",
    "    It loads and processes the event manifest, merges profit and loss (PnL) data with event data, \n",
    "    processes sales data, and combines all datasets for further analysis. \n",
    "    The function also generates patron details, including venue and genre segmentation, \n",
    "    and outputs the final processed data to specified files.\n",
    "\n",
    "Parameters:\n",
    "    manifest_file (str): The file path to the event manifest Excel file.\n",
    "    PnL_file (str): The file path to the Excel file containing PnL data.\n",
    "    PnLProcessed_file (str): The file path where the processed PnL data will be saved as a CSV.\n",
    "    sales_file (str): The file path to the Salesforce sales data CSV file.\n",
    "    regions_file (str): The file path to the CSV file containing regional mapping data.\n",
    "    output_file (str): The file path where the final combined and processed data will be saved as a CSV.\n",
    "\n",
    "Process:\n",
    "    1. **Load Event Manifest**: Loads event data from the manifest file.\n",
    "    2. **Merge PnL Data**: Merges PnL data with event information and writes the processed PnL data to a file.\n",
    "    3. **Load Sales Data**: Loads sales data from the Salesforce report.\n",
    "    4. **Initial Sales Cleanup**: Performs initial cleanup on the sales data, including removing redundant data and filling missing values.\n",
    "    5. **Venue and Attribute Processing**: Processes venue names and adds attributes like chorus membership, student status, and subscription status.\n",
    "    6. **Merge Sales with Event Data**: Combines sales and event data to create a unified dataset.\n",
    "    7. **Genre Segment Processing**: Calculates genre preferences and other segments for each account.\n",
    "    8. **Final Processing and Output**: Cleans up the combined dataset and writes the final processed data to an output file.\n",
    "    9. **Generate Patron Details**: Generates detailed patron data, including RFM scores and segment assignments, and writes this to the specified file.\n",
    "\n",
    "Returns:\n",
    "    None: The processed data is written to the specified output files.\n",
    "\n",
    "Exceptions:\n",
    "    - Catches any exceptions during the execution and logs them.\n",
    "\"\"\"\n",
    "def main(manifest_file,\n",
    "         PnL_file,\n",
    "         PnLProcessed_file,\n",
    "         sales_file, # Updated\n",
    "         regions_file,\n",
    "         output_file): # Updated\n",
    "\n",
    "    # Load event manifest containing event attributes\n",
    "    logger.debug(f'Loading events data')\n",
    "    event_df = mw.load_event_manifest(manifest_file, \n",
    "                                      logger)\n",
    "\n",
    "    # Merge PnL data with manifest and write out PnL_Processed file.\n",
    "    logger.debug(f'Processing PnL data')\n",
    "    event_df = mw.add_PnL_data(event_df,\n",
    "                               PnL_file,\n",
    "                               PnLProcessed_file,\n",
    "                               logger)\n",
    "\n",
    "    # Load sales data from Salesforce report\n",
    "    sales_df = mw.load_sales_file(sales_file, \n",
    "                                  yearsOfData, \n",
    "                                  logger)\n",
    "    \n",
    "    # Initial cleanup of Salesforce data\n",
    "    sales_df = mw.sales_initial_prep(sales_df,\n",
    "                                     AccountId_file, \n",
    "                                     logger)\n",
    "\n",
    "    # Determine venue and intial segments\n",
    "    sales_df = mw.venue_and_attribute_processing(sales_df, \n",
    "                                                 chorus_file, \n",
    "                                                 logger)\n",
    "    \n",
    "    # Merge sales with event attributes\n",
    "    combined_df = mw.combine_sales_and_events(sales_df, \n",
    "                                              event_df, \n",
    "                                              logger)\n",
    "\n",
    "    # Determine genre counts\n",
    "    combined_df = mw.genre_counts(combined_df, \n",
    "                                              logger)\n",
    "\n",
    "    # Final cleanup and write output file\n",
    "    combined_df = mw.final_processing_and_output(combined_df, \n",
    "                                                 output_file, \n",
    "                                                 logger, \n",
    "                                                 processDonations = True)\n",
    "\n",
    "    # Final write patron details file\n",
    "    mw.get_patron_details(combined_df,\n",
    "                          RFMScoreThreshold,\n",
    "                          getZIP,\n",
    "                          regions_file,\n",
    "                          patron_details_file,\n",
    "                          new_threshold,\n",
    "                          returning_threshold,\n",
    "                          logger)\n",
    "\n",
    "#main program\n",
    "try:\n",
    "    start = timer()\n",
    "    if __name__ == '__main__':\n",
    "        \n",
    "        main(manifest_file,\n",
    "             PnL_file,\n",
    "             PnLProcessed_file,\n",
    "             sales_file,\n",
    "             regions_file,\n",
    "             output_file)\n",
    "    end = timer()\n",
    "    timing = timedelta(seconds=(end-start))\n",
    "    formatted_timing = \"{:.2f}\".format(timing.total_seconds())\n",
    "    print(f'Success. Total Time: {formatted_timing}')\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f'Exception caught: {e}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-09-21T15:06:28.706147Z",
     "start_time": "2024-09-21T15:06:10.808074Z"
    }
   },
   "id": "ce7377f24dce9bc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Input files are: EventManifest.xlsx,Budget\\EventPnL.xlsx, SalesforceLatest.csv, final_regions.csv\n",
      "INFO - Output files are: Budget\\EventPnLProcessed.csv, DataMerge.csv\n",
      "INFO - Events loaded. Execution Time: 0.36\n",
      "INFO - PnL data written to Budget\\EventPnLProcessed.csv. Execution Time: 0.10\n",
      "INFO - Starting sales size: (137818, 48)\n",
      "INFO - Pruned sales size: (137818, 48)\n",
      "INFO - Sales loaded. Execution Time: 1.45\n",
      "INFO - Initial sales prep complete. Execution Time: 0.98\n",
      "INFO - Venue and attribute processing complete. Execution Time: 0.35\n",
      "INFO - Sales and events merging. Execution Time: 0.36\n",
      "INFO - genre counts complete. Execution Time: 0.35\n",
      "INFO - Final sales results written to file: DataMerge.csv. Execution Time: 2.84\n",
      "INFO - first/last events added: Execution Time: 1.27\n",
      "INFO - Bulk Buyers added: Execution Time: 0.03\n",
      "INFO - Genre Scores complete. Execution Time: 2.53\n",
      "INFO - Calculating RFM scores...\n",
      "INFO - (18740, 10)\n",
      "INFO - RFM scores complete. Execution Time: 0.06\n",
      "INFO - State and city processing complete. Execution Time: 0.71\n",
      "INFO - address & ZIP processing complete. Execution Time: 0.24\n",
      "INFO - Getting any new Lat/Long data...\n",
      "INFO - 2153 Accounts are missing Lat/Long, likely to bad addresses\n",
      "INFO - 0 new Accounts had Lat/Long added.\n",
      "INFO - Region Processing complete. Execution Time: 0.08\n",
      "INFO - Patron results written to file: PatronDetails.csv\n",
      "INFO - Patron file size: (18753, 38)\n",
      "Success. Total Time: 15.02\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T14:44:33.641555Z",
     "start_time": "2024-09-21T14:44:33.638905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n"
   ],
   "id": "5884536611dfc500",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T14:44:33.645294Z",
     "start_time": "2024-09-21T14:44:33.642573Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "99cace42a7c00c45",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
